# sentiment_train.py
# Usage:
# 1) place sample_data.csv in data/ with columns: text, label (0/1 or multiclass)
# 2) python sentiment_train.py
# Output: saves tfidf_model.pkl (sklearn) and lstm_model.h5 (Keras)

import pandas as pd
import numpy as np
import pickle
import re
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout

def clean_text(s):
    s = s.lower()
    s = re.sub(r'http\S+', '', s)
    s = re.sub(r'@\w+', '', s)
    s = re.sub(r'[^a-z0-9\s]', '', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def train_tfidf_baseline(texts, y):
    vec = TfidfVectorizer(max_features=10000, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("TF-IDF + LogisticRegression accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    with open('tfidf_vectorizer.pkl', 'wb') as f:
        pickle.dump(vec, f)
    with open('tfidf_lr.pkl','wb') as f:
        pickle.dump(clf, f)
    print("Saved tfidf_vectorizer.pkl and tfidf_lr.pkl")

def train_lstm(texts, y, max_words=20000, maxlen=100, embedding_dim=100):
    # tokenize
    tok = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tok.fit_on_texts(texts)
    seqs = tok.texts_to_sequences(texts)
    X = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='post')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # If labels are not numeric 0/1 convert
    num_classes = len(np.unique(y))
    if num_classes == 2:
        loss = 'binary_crossentropy'
        final_activation = 'sigmoid'
    else:
        loss = 'sparse_categorical_crossentropy'
        final_activation = 'softmax'

    model = Sequential([
        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen),
        Bidirectional(LSTM(64, return_sequences=False)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(1 if num_classes==2 else num_classes, activation=final_activation)
    ])
    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])
    model.summary()

    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)
    loss_val, acc_val = model.evaluate(X_test, y_test)
    print("LSTM val accuracy:", acc_val)
    # Save
    model.save('lstm_model.h5')
    with open('tokenizer.pkl','wb') as f:
        pickle.dump(tok, f)
    print("Saved lstm_model.h5 and tokenizer.pkl")

def main():
    df = pd.read_csv('data/sample_data.csv')  # expects columns: text, label
    df['text_clean'] = df['text'].astype(str).apply(clean_text)
    texts = df['text_clean'].tolist()
    y = df['label'].values

    # Baseline TF-IDF + LR
    train_tfidf_baseline(texts, y)

    # LSTM
    train_lstm(texts, y, max_words=20000, maxlen=100, embedding_dim=100)

if __name__ == "__main__":
    main()
